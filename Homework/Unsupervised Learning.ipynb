{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Ideas \n",
    "- Clustering\n",
    "    * explained variance X\n",
    "    * inconsitency, http://cda.psych.uiuc.edu/multivariate_fall_2013/matlab_help/cluster_analysis.pdf X\n",
    "    * Clustering of faces maybe? X\n",
    "    \n",
    "- Dimensionality Reduction\n",
    "    * All Extensions of PCA\n",
    "    * Connection between SVD and PCA\n",
    "    * Look at MAchine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Homework\n",
    "\n",
    "In this notebook you'll deepen your understanding of the unsupervised techniques we've presented with additional material and exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "Let's start by showing one way we can choose the \"correct\" number of clusters.\n",
    "\n",
    "### $k$ Means \"Elbow Method\"\n",
    "\n",
    "Remember that the main idea behind $k$ means clustering is that at each step the algorithm looks to minimize inertia. Where inertia is calculated:\n",
    "$$\n",
    "W = \\sum_{i=1}^k \\sum_{x_j \\in \\text{cluster } i} \\text{dist}(x_j,c_i)^2\n",
    "$$\n",
    "where here $c_i$ is the centroid of cluster $i$.\n",
    "\n",
    "The Elbow Method plots the inertia from the output of the algorithm versus the number of clusters. You then look to identify an \"elbow\" in the curve much like in the explained variance curve from PCA. The elbow is a point at which increasing the number of clusters give diminishing returns in decreasing inertia.\n",
    "\n",
    "I'll show you how to implement it, then you'll need to implement it on a subset of the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saw this in Notebook 1\n",
    "np.random.seed(614)\n",
    "X = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
    "                   [2,2] + 1.7*np.random.randn(100,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function is stored in sklearn.cluster\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the data in the cluster\n",
    "# and the cluster's centroid\n",
    "# then calculates the distance to the centroid\n",
    "def get_dist_to_cent(X,centroid):\n",
    "    return np.sum(np.sum(np.power(X - centroid,2),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = []\n",
    "\n",
    "# for i in 1 to 101\n",
    "for i in range(1,21):\n",
    "    # make a kmeans\n",
    "    kmeans = KMeans(i)\n",
    "\n",
    "    # fit the data\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    # get the cluster\n",
    "    clusters = kmeans.predict(X)\n",
    "\n",
    "    # a holder\n",
    "    temp = 0\n",
    "    # for each cluster\n",
    "    for j in np.unique(clusters):\n",
    "        # get the centroid\n",
    "        j_centroid = kmeans.cluster_centers_[j]\n",
    "        # Add the inertia contribution from the centroid\n",
    "        temp = temp + get_dist_to_cent(X[clusters ==j,:],j_centroid)\n",
    "        \n",
    "    # append the inertia to W\n",
    "    W.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(range(1,21), W)\n",
    "\n",
    "plt.xlabel(\"$k$\", fontsize=16)\n",
    "plt.ylabel(\"Total Inertia\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow is where the rate of decrease in Total Inertia considerably shows. For this data it appears that the elbow is one of $2$, $3$, or $4$.\n",
    "\n",
    "Now it's your turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this first\n",
    "X = pd.read_csv(\"https://raw.githubusercontent.com/cerndb/dist-keras/master/examples/data/mnist.csv\")\n",
    "\n",
    "y = np.array(X.iloc[:,0])\n",
    "X = np.array(X.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subset,X2,y_subset,y2 = train_test_split(X,y,shuffle=True,stratify=y,random_state=440,train_size=2000)\n",
    "\n",
    "del X2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `X_subset` data through the elbow method approach. Note we only subset the data here as a way to decrease the run time. What appears to be the \"correct\" number of clusters for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional techniques for choosing a good number of clusters. I encourage you to read through this reference, <a href=\"https://uc-r.github.io/kmeans_clustering\">https://uc-r.github.io/kmeans_clustering</a>, while the code is in R the concepts are language agnostic. For a package that implements these techniques in python see this package <a href=\"https://www.scikit-yb.org/en/latest/api/cluster/index.html\">https://www.scikit-yb.org/en/latest/api/cluster/index.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering Inconsistency\n",
    "\n",
    "Hierarchical clustering also has a technique in which it attempts to identify the \"natural\" clustering of the data set. We'll describe the approach from page 11-11 on this file <a href=\"http://cda.psych.uiuc.edu/multivariate_fall_2013/matlab_help/cluster_analysis.pdf\">http://cda.psych.uiuc.edu/multivariate_fall_2013/matlab_help/cluster_analysis.pdf</a>.\n",
    "\n",
    "To help explain lets return to this example from Unsupervised Learning Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(440)\n",
    "X = np.random.random((5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the functions are stored\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You input the data\n",
    "# then tell the method how you want measure\n",
    "# closeness\n",
    "# here I've chose 'centroid'\n",
    "Z = linkage(X, 'centroid')\n",
    "\n",
    "# Z returns an array it is called a linkage vector\n",
    "# I'll use a dataframe to examine it\n",
    "# it returns four columns\n",
    "# that I'll describe below\n",
    "pd.DataFrame(Z,columns = ['cluster_1','cluster_2','distance','new_cluster_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can plot the dendrogram like so\n",
    "# I input a linkage vector\n",
    "dendrogram(Z)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two cluster join in a horizontal line we can call this a link. \n",
    "\n",
    "Here I've taken the picture and circled the links\n",
    "<img src=\"DendrogramLinks.png\" style=\"width:60%\"></img>\n",
    "\n",
    "We say that links are <i>consistent</i> if they form at close to the same distance. We say that links are <i>inconsistent</i> if they occur further distances. So in this example the red and and green links appear to be consistent while the blue links appear to be inconsistent with both the red and green links.\n",
    "\n",
    "This notion of inconsistency can then be leveraged to look for a \"natural\" clustering of the data. This is why the dendrogram is colored blue green and red.\n",
    "\n",
    "In `scipy` we can get that natural split using the `fcluster` command with `criterion=inconsistent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcluster(Z,t=.4,criterion='inconsistent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `criterion=inconsistent` version of the algorithm the `t` has a different meaning. You can read what it is from this excerpt of the `fcluster` docs.\n",
    "<img src=\"fclusterDocs.png\" style=\"width:80%\"></img>\n",
    "\n",
    "`scipy` includes an additional function `inconsistent`. Read through page 11-13 of the linked notes and learn how to implement it by reading the docs, <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.inconsistent.html\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.inconsistent.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the natural clustering for this data? Look at the inconsistency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(614)\n",
    "X = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
    "                   [2,2] + 1.7*np.random.randn(100,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional resource on inconsistency that may be useful are these notes from Northwestern, <a href=\"http://www.ece.northwestern.edu/local-apps/matlabhelp/toolbox/stats/multiv15.html\">http://www.ece.northwestern.edu/local-apps/matlabhelp/toolbox/stats/multiv15.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Clustering\n",
    "\n",
    "We noticed some interesting clustering of the MNIST data set. Let's revisit that. One of your classmates pointed out that her clustering of these digits gave this weird cluster:\n",
    "\n",
    "<b>Cluster 2</b>\n",
    "\n",
    "Digit - Number of Observations\n",
    "\n",
    "4  -  2279\n",
    "\n",
    "9  -  2115\n",
    "\n",
    "7  - 1310\n",
    "\n",
    "\n",
    "Now why would that be?\n",
    "\n",
    "Rerun Clustering on the real MNIST data set.\n",
    "\n",
    "Look in your results for a weird cluster like the one produced by your classmate. Then look at the digits within the cluster as images. Can you identify any reasons as to why the algorithm grouped these seemingly different numbers together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "### Pearson's Original Formulation of PCA\n",
    "\n",
    "Karl Pearson's 1903 paper, <a href = \"https://www.tandfonline.com/doi/abs/10.1080/14786440109462720\"><i>On lines and planes of closest fit to systems of points in space</i></a>, was the original introduction of PCA.\n",
    "\n",
    "In this paper Pearson contemplated what we mean by line (or plane) of best fit. In regression the approach is to find the line of best fit by minimizing the distance between the estimate, $\\hat{y}$, and the actual value, $y$. Pearson wondered what if instead we considered minimizing the distance between the data points and the hyperplane fit to those points.\n",
    "\n",
    "Consider this image from Pearson's 1903 paper.\n",
    "\n",
    "<img src = \"PearsonLine.png\" width = \"500\"></img>\n",
    "\n",
    "It turns out that this formulation is equivalent to maximizing the variance.\n",
    "\n",
    "\n",
    "Read through page 352 here, <a href=\"https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\">https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf</a> for a derivation of that fact.\n",
    "\n",
    "### The Relationship of PCA to SVD\n",
    "\n",
    "Recall that the PCA component vectors are given by the eigenvectors of $\\Sigma$, the covariance matrix of the data matrix $X$, where the columns of $X$ have all been centered at $0$.\n",
    "\n",
    "Any covariance matrix is symmetric, and thus is diagonalizable. So $\\Sigma = Q^T \\Lambda Q$ for some matrix $Q$. \n",
    "\n",
    "In a potential abuse of notation let $X = U \\sigma V^T$ be the singular value decomposition of $X$.\n",
    "\n",
    "Now recall that the sample covariance matrix can be computed as $X^T X$.\n",
    "\n",
    "Recalculate $X^T X$ substituting in the SVD of $X$, what do you find? Hint: $V$ is orthonormal.\n",
    "\n",
    "\n",
    "###### Spoiler Below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "It is actually much safer to calculate the SVD of $X$ rather than find the eigenvectors of $X^T X$, and this is what `sklearn` does.\n",
    "\n",
    "\n",
    "### PCA Handle Multicolinearity\n",
    "\n",
    "Here's a very simple example of how PCA can be helpful in regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((1000,2))\n",
    "\n",
    "X[:,0] = 4*np.random.randn(1000)\n",
    "X[:,1] = 2*X[:,0] + np.random.randn(1000)\n",
    "\n",
    "y = X[:,0] + 12 + X[:,1] + 1.5*np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x1':X[:,0],'x2':X[:,1],'y':y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine `df` for multicolinearity.\n",
    "\n",
    "Then apply PCA to `X` using 2 components. Add the PCA transformed `X` as new columns of `df`.\n",
    "\n",
    "Examine those columns for multicolinearity and their relationship with `y`.\n",
    "\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions of PCA\n",
    "\n",
    "We now survey various extensions of PCA and provide why they're useful.\n",
    "\n",
    "#### Sparse PCA\n",
    "\n",
    "Think of this as the Lasso Regression of PCA techniques <a href=\"https://en.wikipedia.org/wiki/Sparse_PCA\">https://en.wikipedia.org/wiki/Sparse_PCA</a>.\n",
    "\n",
    "In this setting you're maximizing the variance along the project $w$ while using these new constraints, $||w||_2^2 = 1$ and $||w||_0 < k$, where $k$ is chosen ahead of time and $||w||_0 = |w_1|^0 + |w_2|^0 + \\dots + |w_m|^0$\n",
    "\n",
    "The idea here is that similar to the issue with ridge regression the components of normal PCA tend to have nonzero entries for all of the $m$ features. Like with Lasso the $||w||_0 < k$ constraint forces some number of them to be $0$.\n",
    "\n",
    "This can be implemented with `SparsePCA` in `sklearn`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html\">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html</a>.\n",
    "\n",
    "Return to the faces example in notebook 5. Does anything change if you use `SparsePCA` instead of `PCA`? I genuinely don't know what will happen but I'm curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized PCA\n",
    "\n",
    "This version of PCA takes advantage of randomization in order to make low order PCA projects much faster on very high dimensional data. Here is the paper where it was introduced <a href=\"https://arxiv.org/abs/0909.4061\">https://arxiv.org/abs/0909.4061</a>.\n",
    "\n",
    "You can implement this with `PCA(svd_solver=\"randomized\")`.\n",
    "\n",
    "We haven't worked with data sets large enough to necesitate its use, but you should be aware of its existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA\n",
    "\n",
    "Kernel PCA is PCA that is able to utilize the kernel trick from support vector machines, <a href=\"https://en.wikipedia.org/wiki/Kernel_principal_component_analysis\">https://en.wikipedia.org/wiki/Kernel_principal_component_analysis</a>.\n",
    "\n",
    "The idea is, once again, to lift our data into a higher dimensional space (typically a Hilbert space) and then perform PCA up there, then bring it back down to lower dimensions. Where we utilize the kernel trick as a way to avoid the higher dimensional space all together.\n",
    "\n",
    "We can use this on data sets where typical PCA may fail to some success.\n",
    "\n",
    "Let's return to the HELLO data from notebook 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hello(N=1000, rseed=42):\n",
    "    # Make a plot with \"HELLO\" text; save as PNG\n",
    "    fig, ax = plt.subplots(figsize=(4, 1))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85)\n",
    "    fig.savefig('hello.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Open this PNG and draw random points from it\n",
    "    from matplotlib.image import imread\n",
    "    data = imread('hello.png')[::-1, :, 0].T\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(4 * N, 2)\n",
    "    i, j = (X * data.shape).astype(int).T\n",
    "    mask = (data[i, j] < 1)\n",
    "    X = X[mask]\n",
    "    X[:, 0] *= (data.shape[0] / data.shape[1])\n",
    "    X = X[:N]\n",
    "    return X[np.argsort(X[:, 0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our data\n",
    "X = make_hello(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it in 2D\n",
    "colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5), s = 20)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], **colorize)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hello_s_curve(X):\n",
    "    t = (X[:, 0] - 2) * 0.75 * np.pi\n",
    "    x = np.sin(t)\n",
    "    y = X[:, 1]\n",
    "    z = np.sign(t) * (np.cos(t) - 1)\n",
    "    return np.vstack((x, y, z)).T\n",
    "\n",
    "XS = make_hello_s_curve(X)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2],\n",
    "             **colorize);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA on the `XS` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try `KernelPCA` from `sklearn` using different kernel functions <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html</a>, does it perform any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Political Data\n",
    "\n",
    "#### Senator Votes\n",
    "\n",
    "Here's a fun example using some political data. Note you'll need to unzip this data first, it is stored in SenateVotes.zip.\n",
    "\n",
    "Read in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_112 = pd.read_csv(\"Senate_112_roll_mat.csv\")\n",
    "\n",
    "congress_112.loc[congress_112.party_code == 200, \"party_code\"] = \"Republican\"\n",
    "congress_112.loc[congress_112.party_code == 100, \"party_code\"] = \"Democrat\"\n",
    "congress_112.loc[congress_112.party_code == 328, \"party_code\"] = \"Independent\"\n",
    "\n",
    "congress_112.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains votes for every senator from the 112th Congress. Each row is a senator and each column to the right after `party_code` is a roll call vote conducted in the Senate.\n",
    "\n",
    "Run the votes through PCA, project down to 2 dimensions. Plot the results, what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the 113th, 114th, and 115th senates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Polarization\n",
    "\n",
    "A great fear of social media is that it is leading to greater polarization of our society, but what does this have to do with PCA?\n",
    "\n",
    "Well lets look at a data set collected by the Center for the Study of Networks and Society (CSNS). For a <a href = \"http://www.susanbourbaki.com/category/breitbart/\">blog post</a> back in 2017 in which they examined the following habits of various Twitter users they collected every follower from the following 30 accounts: DrDavidDuke, AryanBrother, CofCC76, Hatchet318, KKKOfficial311, KeyStoneUnited, MatthewHeimbach, ThaRightStuff, nsm88, BreitbartNews, FiveThirtyEight, csmonitor, MotherJones, NRO, dailykos, theblaze, thenation, DRUDGE_REPORT, WSJ, washingtonpost, FoxNews, NPR, realDonaldTrump, tedcruz, BernieSanders, SenWarren, SpeakerRyan, SenJohnMcCain, marcorubio, and CoryBooker.\n",
    "\n",
    "Now we will depart from the story told by CSNS and look at a specific subset of their data, just those followers of the 13 media accounts: BreitbartNews, FiveThirtyEight, csmonitor, MotherJones, NRO, dailykos, theblaze, thenation, DRUDGE_REPORT, WSJ, washingtonpost, FoxNews, NPR. However, I encourage you to read their blog post as well as their recent paper on arxiv, <a href = \"https://arxiv.org/pdf/1905.07755.pdf\"><i>Online reactions to the 2017 ‘Unite the Right’ rally in Charlottesville: Measuring polarization in Twitter networks using media followership</i></a> that includes a more in depth analysis of this data in the context of the Charlottesville incident.\n",
    "\n",
    "Our goal will be to see if PCA can illuminate any information on the anonymized Twitter users based on their media account followership. One more additional caveat before we move forward, the original media restricted dataset contains 27,056,206 unique Twitter accounts. We will only look at a stratified sample of this data that contains only 100,000 accounts. The data has been sampled in such a way that we retain the results found by CSNS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "twitter = pd.read_csv(\"twitter_sba.csv\")\n",
    "\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before performing PCA let's look at the believed political leanings of each of our media accounts.\n",
    "\n",
    "| Twitter Screenname | News Source               | Leaning Tendency |\n",
    "|--------------------|---------------------------|------------------|\n",
    "| FoxNews            | Fox News                  | Right            |\n",
    "| BreitbartNews      | Breitbart                 | Right            |\n",
    "| DRUDGE_REPORT      | The Drudge Report         | Right            |\n",
    "| the blaze          | TheBlaze                  | Right            |\n",
    "| NRO                | National Review           | Right            |\n",
    "| WSJ                | Wall Street Journal       | center           |\n",
    "| csmonitor          | Christian Science Monitor | left             |\n",
    "| FiveThirtyEight    | FiveThirtyEight           | left             |\n",
    "| dailykos           | Daily KOS                 | left             |\n",
    "| thenation          | The Nation                | left             |\n",
    "| MotherJones        | Mother Jones              | left             |\n",
    "| washingtonpost     | The Washington Post       | left             |\n",
    "| NPR                | NPR                       | left             |\n",
    "\n",
    "These tendencies have been corroborated by peer reviewed studies. The Wall Street Journal is labeled as center because it is widely considered to be conservative-leaning, but has been grouped as liberal based on readership and co-citations.\n",
    "\n",
    "Run this data through PCA. Look at the principal component vectors, what do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Learning\n",
    "\n",
    "### tSNE Blobs\n",
    "\n",
    "Look at this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First blob\n",
    "A = np.random.normal(scale = 1, size = (300,3))\n",
    "\n",
    "# The blob that surrounds A\n",
    "B = np.array([x for x in np.random.normal(scale = 5, size = (1000,3)) if np.linalg.norm(x) > 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(A[:,0], A[:,1], A[:,2])\n",
    "ax.scatter(B[:,0], B[:,1], B[:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the data\n",
    "X = np.r_[A,B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `X` through PCA and plot in 2D. Then run `X` through tSNE and plot in 2D. Which is better at separating the points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies\n",
    "\n",
    "From the following kaggle data set, https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset/downloads/imdb-5000-movie-dataset.zip/1, I extracted the movie name and the genres. From that data I created the following dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 26 possible genres, and for each genre column there is a 0 or 1 depending on whether or not that movie was classified as that genre.\n",
    "\n",
    "Using different manifold learning techniques explore if there are natural genre groupings that many films fall into. You could look at single genres like Documentary, or combinations like Fantasy Adventure. Consider adding a binary variable for a film series like Star Wars, do those films go to the same place in the plot?\n",
    "\n",
    "There are no right or wrong answers here, just have fun exploring the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
