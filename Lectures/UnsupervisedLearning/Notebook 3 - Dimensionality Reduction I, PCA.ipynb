{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction I, PCA\n",
    "\n",
    "We start off dimensionality reduction by discussing perhaps the most popular dimensionaliy reduction technique in data science, principal components analysis (PCA).\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "We'll:\n",
    "<ul>\n",
    "    <li>Briefly discuss the need or desire for dimensionality reduction,</li>\n",
    "    <li>Introduce the concept and mathematics behind PCA,</li>\n",
    "    <li>Implement PCA on the Wisconsin cancer set,</li>\n",
    "    <li>Show how to interpret the output of PCA by examining the component vectors with basketball data,</li>\n",
    "    <li>Introduce the explained variance curve.</li>\n",
    "</ul>\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But Why Dimensionality Reduction?\n",
    "\n",
    "There are a number of reasons! Here are a few:\n",
    "\n",
    "1. Reducing the number of dimensions is a way we can eliminate noise variables from our data allowing for better, we'll see an example of this with the Wisconsin Cancer data.\n",
    "\n",
    "2. Your data is too big. This is common with image classification problems like the MNIST data.\n",
    "\n",
    "3. Humans can only see in 2 or 3 dimensions, and interesting data sets often have many more dimensions than that.\n",
    "\n",
    "\n",
    "I've hopefully convinced you of the importance of dimensionality reduction. If not, well that's too bad, humor me anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "PCA is one of the most popular dimensionality reduction techniques. So much so that we may take an entire day on just this topic, sorry manifold learning and t-SNE.\n",
    "\n",
    "This technique has been around since Karl Pearson's 1903 paper, <a href = \"https://www.tandfonline.com/doi/abs/10.1080/14786440109462720\"><i>On lines and planes of closest fit to systems of points in space</i></a>.\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "When you reduce the dimension of a data set you are inherently losing information. Therefore you want to ensure that you do it in a way that \"retains as much information as possible\". PCA tackles this problem in a very statistical manner.\n",
    "\n",
    "There's an idea in statistics that the information of a data set is located within that data set's variation. Thus when you reduce the dimension of a data set, you want to project your data onto a lower dimensional space that captures as much of the original variance in the data as possible. Thinking in terms of optimization, your goal is to project into a lower dimensional hyperplane in a way that maximizes variance.\n",
    "\n",
    "Here's a heuristic algorithm:\n",
    "1. Center your data so that each feature has 0 mean, this is done for convenience.\n",
    "2. Find the direction in space along which projections have the highest variance, this produces the first principal component.\n",
    "3. Find the direction orthogonal to the first principal component that maximizes variance, this is the second principal component.\n",
    "4. Continue in this way, the kth principal component is the variance-maximizing direction orthogonal to the previous k-1 components.\n",
    "\n",
    "Let's see what we mean in a 2-D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some random data\n",
    "np.random.seed(440)\n",
    "\n",
    "x1 = 8*np.random.randn(500)\n",
    "x2 = 3*np.random.randn(500)\n",
    "\n",
    "X = np.concatenate([x1.reshape(-1,1),x2.reshape(-1,1)], axis = 1)\n",
    "\n",
    "angle = -np.pi/4\n",
    "\n",
    "X = X.dot(np.array([[np.cos(angle),-np.sin(angle)],[np.sin(angle),np.cos(angle)]]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is stored in decomposition\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the PCA object\n",
    "# we'll project down to 2-D\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit the data\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, \n",
    "                    shrinkB=0,\n",
    "                    color=\"black\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=.6)\n",
    "\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Math, The Myth, The Legend\n",
    "\n",
    "Now let's get down to the mathematical details. We consider the maximal variance formulation of the problem, for those interested in the original problem statement proposed by Pearson check out the Homework.\n",
    "\n",
    "Suppose we have $n$ observations of $m$ features, let $X_1,X_2,\\dots,X_m$ be $n$ by $1$ vectors containing the observations of each of the $m$ features. And for ease of notation assume each has been centered to have mean $0$.\n",
    "\n",
    "We'll restrict ourselves to the case of finding the first principal component, the others can be found in a similar fashion.\n",
    "\n",
    "Let \n",
    "$$\n",
    "X = \\left(X_1 | X_2 | \\dots |X_m \\right)\n",
    "$$\n",
    "be an $n$ by $m$ feature matrix.\n",
    "\n",
    "Our goal is to find $w=(w_1,w_2,\\dots,w_m)^T$ so that $\\text{Var}(w_1 X_1 + w_2 X_2 + \\dots + w_m X_m) = \\text{Var}(w^T X)$ is maximized. Because we have centered the columns of $X$ we have:\n",
    "$$\n",
    "\\text{Var}(w^T X) = E(w^T X X^T w) = w^T E(X X^T) w = w^T \\Sigma w,\n",
    "$$\n",
    "where $\\Sigma$ is the covariance matrix of $X$. In order to ensure our $w$ is finite we impose the constraint that $||w|| = 1$. To solve the constrained optimization problem we use Lagrange multipliers where $f(w) = w^t \\Sigma w$ and $g(w) = w^T w - 1$. Using matrix calculus (see the references below for a helpful resource):\n",
    "$$\n",
    "\\partial_w \\left(w^T \\Sigma w - \\lambda (w^T w - 1)\\right) = 2 \\Sigma w - 2\\lambda w.\n",
    "$$\n",
    "\n",
    "Setting this equal to $0$ and solving gives\n",
    "$$\n",
    "\\Sigma w = \\lambda w,\n",
    "$$\n",
    "the standard eigenvalue setup.\n",
    "\n",
    "So the vector $w$ that maximizes variance is an eigenvector corresponding to the largest eigenvector of the covariance matrix of $X$.\n",
    "\n",
    "This vector is known as the first principal component, and as luck would have it the remaining principal components correspond to the remaining eigenvectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Cancer Again\n",
    "\n",
    "Now let's get some practice implementing PCA to help with supervised learning.\n",
    "\n",
    "We return once again to classifying cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X,y = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "# test train split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .25,random_state = 614,shuffle = True,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's view the training data\n",
    "# Note this may take a bit.\n",
    "fig, axes = plt.subplots(15, 2, figsize = (12,30))\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(30):\n",
    "    _, bins = np.histogram(X_train[:, i], bins = 50)\n",
    "    ax[i].hist(X_train[y_train == 1, i], bins = bins, color = 'red', alpha = .5)\n",
    "    ax[i].hist(X_train[y_train == 0, i], bins = bins, color = 'blue', alpha = .5)\n",
    "    ax[i].set_title(\"feature \" + str(i) + \": \" + cancer.feature_names[i])\n",
    "    ax[i].set_yticks(())\n",
    "\n",
    "ax[0].legend(['malignant', 'benign'], loc = 'best')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using our intuition and explatory data analysis let's build a model implementing PCA as an added preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe = Pipeline([('scaler',StandardScaler()),\n",
    "                ('pca',PCA(n_components=2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca_pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.scatter(X_pca[y_train==0,0],X_pca[y_train==0,1],c='blue',label='benign')\n",
    "plt.scatter(X_pca[y_train==1,0],X_pca[y_train==1,1],c='red',label='malignant')\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"pca 1\",fontsize=16)\n",
    "plt.ylabel(\"pca 2\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Combine PCA with one of our previous classification techniques to build a cancer classifier. If you have time implement cross validation and get a sense of how well we should expect this approach to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's Nice, But What Does it Mean?\n",
    "\n",
    "So we've seen that PCA can be quite useful. However, many of you might be wondering how we can interpret PCA results.\n",
    "\n",
    "Well let's see.\n",
    "\n",
    "### 3 > 2\n",
    "\n",
    "We return to the basketball court. Basketball courts can be sliced into 15 distinct regions, you can then take all of a team's shot attempts and bin them according to the region where they were taken, if you then divide by their total shots taken you've got yourself a team's shot distribution. Below is an image of the court cut into 15 regions.\n",
    "\n",
    "<img src=\"CourtZones.png\" style=\"width:50%;\"></img>\n",
    "\n",
    "We now load in data from the 2000-01 and 2018-19 seasons. We'll run this through PCA extracting two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = pd.read_csv(\"nba_team_shots.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a PCA object\n",
    "pca = PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it on the desired columns\n",
    "nba_pca = pca.fit_transform(nba[['zone_' + str(i) + '_perc_att' for i in range(1,16)]])\n",
    "\n",
    "nba['pca_1'] = nba_pca[:,0]\n",
    "nba['pca_2'] = nba_pca[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the projected values we can examine the projection vectors ($w$ from earlier) called the components.\n",
    "\n",
    "We'll do this with a sorted data frame, and a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'pca_1':pca.components_[0,:],\n",
    "              'pca_2':pca.components_[1,:]},\n",
    "            index = ['zone_' + str(i) + '_perc_att' for i in range(1,16)]).sort_values('pca_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'pca_1':pca.components_[0,:],\n",
    "              'pca_2':pca.components_[1,:]},\n",
    "            index = ['zone_' + str(i) + '_perc_att' for i in range(1,16)]).sort_values('pca_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize with a heatmap\n",
    "sns.set_style(\"white\")\n",
    "plt.matshow(pca.components_, cmap = 'plasma')\n",
    "\n",
    "plt.yticks([0,1], [\"First Component\", \"Second Component\"],\n",
    "          fontsize = 14)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xticks(range(15),\n",
    "          ['zone_' + str(i) + '_perc_att' for i in range(1,16)], rotation = 90, \n",
    "          fontsize = 14)\n",
    "\n",
    "plt.xlabel(\"Feature\", fontsize = 14)\n",
    "plt.ylabel(\"Principal Components\", fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these two vectors mean? For any principal component vector, positive (meaning greater than 0) vector entries correspond to more positive principal component values. For example a team with 100% of their shots from Zone 12 will have as negative a first principal component value as possible because that is the most negative row of the first principal component vector table. On the other hand, a team that shot 100% of their shots from Zone 5 will have as positive a first principal component value as possible because that is the most positive row of the first principal component vector table. \n",
    "\n",
    "In this way we can read through the principal components and see which features are being picked up in that component. We can also see how this gets reflected in the PCA plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(nba.loc[nba.season == '2000-01','pca_1'].values,\n",
    "            nba.loc[nba.season == '2000-01','pca_2'].values,\n",
    "            color = 'orange', label = \"2000-01 Season\")\n",
    "\n",
    "plt.scatter(nba.loc[nba.season == '2018-19','pca_1'].values,\n",
    "            nba.loc[nba.season == '2018-19','pca_2'].values,\n",
    "            color = 'blue', label = \"2018-19 Season\")\n",
    "\n",
    "plt.xlabel(\"PCA 1\", fontsize=16)\n",
    "plt.ylabel(\"PCA 2\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot tells us that NBA teams from 20 years ago (for the most part) had a much different approach to the game. This reflects the shocking revalation that <a href=\"https://fivethirtyeight.com/features/how-mapping-shots-in-the-nba-changed-it-forever/\">$3$ points is worth more than $2$</a>.\n",
    "\n",
    "If you're interested in learning more check out my blog post (a shameless plug) on how we can use PCA to track changes in NBA shot distributions over time <a href=\"http://matthew-osborne.com/mtodata/Posts/PCA-in-NBA.html\">http://matthew-osborne.com/mtodata/Posts/PCA-in-NBA.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Examine the component vectors from the Wisconsin Cancer data. What features about the tumor image seem important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Many Components? The Explained Variance Curve\n",
    "\n",
    "Another question you may want to answer is how many components should I use?\n",
    "\n",
    "This is where something called the <i>explained variance curve</i> comes into play. sklearn's PCA function does give us the tools to figure out how much of the variance each principal component explains. This is known as the explained variance ratio of the component and is available via the explained_variance_ratio_ variable. These values indicate the proportion of the original dataset's variance that lies along the axis of each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return once more to the MNIST data set, the real one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"https://raw.githubusercontent.com/cerndb/dist-keras/master/examples/data/mnist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "\n",
    "pca.fit(X[X.columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(1,201),\n",
    "        np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.xlabel(\"PCA Component\", fontsize=16)\n",
    "plt.ylabel(\"Cumulative Variance Explained\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the original data set has $784$ features, however we are able to capture over $80\\%$ of the variance with fewer than $50$.\n",
    "\n",
    "One rule of thumb for choosing the dimension of the hyperplane is to look for the \"elbow\" in the cumulative explained variance plot. This is where the explained variance ratio stops growing as quickly and is thought of as the \"intrinsic dimensionality\" of the dataset. This looks to be at around 6 or 7 dimensions for the cancer data set.\n",
    "\n",
    "An additional feature of `sklearn` PCA is that you can tell it what percentage of the variance you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=.5)\n",
    "\n",
    "pca.fit(X[X.columns[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Examine the Explained Variance Ratio for the Wisconsin Cancer Data Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this notebook! I encourage you to check out the Unsupervised Learning Homework where we introduce common extensions of PCA for various types of data as well as show how you can use PCA to address multicolinearity.\n",
    "\n",
    "See you in Notebook 4 where we introduce Manifold techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "<a href = \"https://www.tandfonline.com/doi/abs/10.1080/14786440109462720\">On lines and planes of closest fit to systems of points in space</a>\n",
    "\n",
    "<a href=\"https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\">Univeristy of Waterloo Matrix Cookbook</a>\n",
    "\n",
    "<a href=\"http://www.math.kent.edu/~reichel/courses/monte.carlo/alt4.7d.pdf\">Kent State University Notes on Random Vectors and Matrices</a>\n",
    "\n",
    "<a href=\"http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/pca.pdf\">Columbia PCA notes</a>\n",
    "\n",
    "<a href=\"https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\">Central Michigan PCA notes</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
